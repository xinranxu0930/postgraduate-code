{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from subprocess import call,run\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import re\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_qtl(qtl_file,qtl_type):\n",
    "    if qtl_type == \"stQTL\":\n",
    "        cols_to_read = [\"chrom\",\"strand\",\"snp_pos_1base\",\"rsID\",\"pvalue\"]\n",
    "        qtl_data = pd.read_csv(qtl_file, usecols=cols_to_read)\n",
    "        qtl_filtered = qtl_data[qtl_data['pvalue'] < 0.05]\n",
    "        qtl_filtered['s'] = qtl_filtered['snp_pos_1base']-1\n",
    "        qtl_filtered['e'] = qtl_filtered['snp_pos_1base']+1\n",
    "        return qtl_filtered[['chrom', 's', 'e', 'rsID', 'pvalue', 'strand']]\n",
    "    else:\n",
    "        cols_to_read = [\"chrom\",\"strand\",\"snp_pos_1base\",\"rsID\",\"BayesFactor\"]\n",
    "        qtl_data = pd.read_csv(qtl_file, usecols=cols_to_read)\n",
    "        qtl_filtered = qtl_data[qtl_data['BayesFactor'] > 3]\n",
    "        qtl_filtered['s'] = qtl_filtered['snp_pos_1base']-1\n",
    "        qtl_filtered['e'] = qtl_filtered['snp_pos_1base']+1\n",
    "        return qtl_filtered[['chrom', 's', 'e', 'rsID', 'BayesFactor', 'strand']] # chrom   start   end   name   score   strand\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lead_qtl(qtl_genes_bed,qtl_type):\n",
    "    qtl_genes = pd.read_csv(qtl_genes_bed, sep='\\t', header=None,usecols=[0,1,3,4,9])\n",
    "    if qtl_type == \"stQTL\":\n",
    "        qtl_genes.columns = ['chrom', 'snp_pos', 'rsID', 'pvalue', 'geneID']\n",
    "        qtl_genes['snp_pos'] = qtl_genes['snp_pos']+1\n",
    "        lead_qtls = qtl_genes.loc[qtl_genes.groupby('geneID')['pvalue'].idxmin()]\n",
    "    else:\n",
    "        qtl_genes.columns = ['chrom', 'snp_pos', 'rsID', 'BF', 'geneID']\n",
    "        qtl_genes['snp_pos'] = qtl_genes['snp_pos']+1\n",
    "        # 对每个基因（geneID）分组，保留BayesFactor（BF）最大的那个QTL\n",
    "        lead_qtls = qtl_genes.loc[qtl_genes.groupby('geneID')['BF'].idxmax()]\n",
    "    return lead_qtls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_leadQTL_distance(qtl_file_1, qtl_file_2, qtl1, qtl2, gene_bed_file, chrom_size_file, work_dir, bim_dict, bfile_path):\n",
    "    # 设置工作目录\n",
    "    os.chdir(work_dir)\n",
    "\n",
    "    # Step 1: 读取并筛选QTL summary数据\n",
    "    qtl_filtered_1 = load_and_filter_qtl(qtl_file_1,qtl1)\n",
    "    qtl_filtered_2 = load_and_filter_qtl(qtl_file_2,qtl2)\n",
    "\n",
    "    # 保存为BED文件\n",
    "    qtl_filtered_1.to_csv('qtl_1.bed', sep='\\t', header=False, index=False)\n",
    "    qtl_filtered_2.to_csv('qtl_2.bed', sep='\\t', header=False, index=False)\n",
    "\n",
    "    # Step 2: 排序BED文件\n",
    "    call(f\"sort -t, -k1,1V -k2,2n qtl_1.bed > qtl_sorted_1.bed\",shell=True)\n",
    "    call(f\"sort -t, -k1,1V -k2,2n qtl_2.bed > qtl_sorted_2.bed\",shell=True)\n",
    "\n",
    "    # Step 3: 使用bedtools intersect找出QTL所在的基因\n",
    "    call(f\"bedtools intersect -a qtl_sorted_1.bed -b {gene_bed_file} -g {chrom_size_file} -wa -wb -s -sorted > qtl_1_gene.bed\",shell=True)\n",
    "    call(f\"bedtools intersect -a qtl_sorted_2.bed -b {gene_bed_file} -g {chrom_size_file} -wa -wb -s -sorted > qtl_2_gene.bed\",shell=True)                  \n",
    "\n",
    "    # Step 4: 读取QTL和基因信息并找到每个基因的lead QTL\n",
    "    lead_qtls_1 = get_lead_qtl('qtl_1_gene.bed',qtl1)\n",
    "    lead_qtls_2 = get_lead_qtl('qtl_2_gene.bed',qtl2)\n",
    "    lead_qtls_1 = lead_qtls_1.rename(columns={'rsID': 'rsID_1', 'snp_pos': 'snp_pos_1'})\n",
    "    lead_qtls_2 = lead_qtls_2.rename(columns={'rsID': 'rsID_2', 'snp_pos': 'snp_pos_2'})\n",
    "    merged_qtls = pd.merge(lead_qtls_1, lead_qtls_2[['geneID', 'rsID_2', 'snp_pos_2']], on='geneID', how='inner')\n",
    "\n",
    "    # Step 5: 计算两个lead QTL之间的距离\n",
    "    merged_qtls['dis'] = abs(merged_qtls['snp_pos_2'] - merged_qtls['snp_pos_1'])\n",
    "\n",
    "    # 创建新的列 dis_group 根据预设的区间来划分 dis\n",
    "    bins = [0, 10, 100, 1000, 10000, 100000]  # 设置区间\n",
    "    labels = ['0', '10', '100', '1000', '10000']\n",
    "    merged_qtls['dis_group'] = pd.cut(merged_qtls['dis'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "    # 设置绘图样式\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    # 绘制柱状图\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.countplot(data=merged_qtls, x='dis_group', color='skyblue', edgecolor='black',width=0.4)\n",
    "    # 添加标题和标签\n",
    "    # plt.title('Distribution of Lead SNP Distance by Range', fontsize=16)\n",
    "    plt.xlabel(f'Distance Range Between {qtl1} and {qtl2} (bp)', fontsize=12)\n",
    "    plt.ylabel('Count', fontsize=12)\n",
    "    # 调整布局\n",
    "    plt.tight_layout()\n",
    "    # 保存图表为PDF\n",
    "    plt.savefig(f'{qtl1}_{qtl2}_leaddis.pdf')\n",
    "\n",
    "    # 检查是否包含 \"rs\"\n",
    "    mask_rsID_1 = merged_qtls['rsID_1'].str.contains('rs', na=False)\n",
    "    mask_rsID_2 = merged_qtls['rsID_2'].str.contains('rs', na=False)\n",
    "\n",
    "    # 将 snp_pos_1 和 snp_pos_2 列转换为字符串类型\n",
    "    merged_qtls['snp_pos_1'] = merged_qtls['snp_pos_1'].astype(str)\n",
    "    merged_qtls['snp_pos_2'] = merged_qtls['snp_pos_2'].astype(str)\n",
    "\n",
    "    # 使用位置信息替换不包含 \"rs\" 的值\n",
    "    merged_qtls.loc[~mask_rsID_1, 'rsID_1'] = merged_qtls.loc[~mask_rsID_1, 'chrom'].str.replace('ch', '') + \"_\" + merged_qtls.loc[~mask_rsID_1, 'snp_pos_1'].str[2:]\n",
    "    merged_qtls.loc[~mask_rsID_2, 'rsID_2'] = merged_qtls.loc[~mask_rsID_2, 'chrom'].str.replace('ch', '') + \"_\" + merged_qtls.loc[~mask_rsID_2, 'snp_pos_2'].str[2:]\n",
    "    # 替换rsID为bim中的SNP name\n",
    "    merged_qtls['rsID_1'] = merged_qtls['rsID_1'].map(bim_dict)\n",
    "    merged_qtls['rsID_2'] = merged_qtls['rsID_2'].map(bim_dict)\n",
    "    merged_qtls = merged_qtls.dropna(subset=['rsID_1', 'rsID_2'])\n",
    "    merged_qtls = merged_qtls.reset_index(drop=True)\n",
    "    res_df = add_ld_to_dataframe(merged_qtls, bfile_path)\n",
    "    res_df.to_csv(f'{qtl1}_{qtl2}_LD.csv', index=False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_bed_file = '/mnt/hpc/home/xuxinran/DirectSeq/8_downsteam/GeneFunctionalPathways/gff_sorted.bed'\n",
    "chrom_size_file = '/mnt/hpc/home/xuxinran/DirectSeq/8_downsteam/GeneFunctionalPathways/hg19.chrom.sizes.sorted'\n",
    "work_dir = '/mnt/hpc/home/xuxinran/DirectSeq/8_downsteam/leadSNP_dis_LD'\n",
    "bfile_path = '/mnt/hpc/home/xuxinran/huvec_genotype/huvec_imputed'\n",
    "\n",
    "\n",
    "qtl_files = [\n",
    "    '/mnt/hpc/home/xuxinran/DirectSeq/data/zhaolin_240206/240201-zhaolin-RNA-merge/v0.8.1/Iqtl/nano_merge_I_summary.csv',\n",
    "    '/mnt/hpc/home/xuxinran/DirectSeq/data/zhaolin_240206/240201-zhaolin-RNA-merge/v0.8.1/puqtl/nano_merge_promoter_summary.csv',\n",
    "    '/mnt/hpc/home/xuxinran/DirectSeq/data/zhaolin_240206/240201-zhaolin-RNA-merge/v0.8.1/m6Aqtl/nano_merge_m6A_summary.csv',\n",
    "    '/mnt/hpc/home/xuxinran/DirectSeq/data/zhaolin_240206/240201-zhaolin-RNA-merge/v0.8.1/pseUqtl/nano_merge_pseU_summary.csv',\n",
    "    '/mnt/hpc/home/xuxinran/DirectSeq/data/zhaolin_240206/240201-zhaolin-RNA-merge/v0.8.1/m5Cqtl/nano_merge_m5C_summary.csv',\n",
    "    '/mnt/hpc/home/xuxinran/DirectSeq/data/zhaolin_240206/240201-zhaolin-RNA-merge/v0.8.1/stqtl/nano_merge_stability_summary.csv',\n",
    "    '/mnt/hpc/home/xuxinran/DirectSeq/data/zhaolin_240206/240201-zhaolin-RNA-merge/v0.8.1/3aqtl/nano_merge_APA_summary.csv',\n",
    "    '/mnt/hpc/home/xuxinran/DirectSeq/data/zhaolin_240206/240201-zhaolin-RNA-merge/v0.8.1/irqtl/nano_merge_isoform_summary.csv'\n",
    "]\n",
    "qtl_names = [\n",
    "    'inosine-QTL',\n",
    "    'puQTL',\n",
    "    'm6A-QTL',\n",
    "    'pseU-QTL',\n",
    "    'm5C-QTL',\n",
    "    'stQTL',\n",
    "    '3aQTL',\n",
    "    'irQTL'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成sh文件\n",
    "res = []\n",
    "\n",
    "for i, j in itertools.combinations(range(len(qtl_names)), 2):\n",
    "        qtl_file_1 = qtl_files[i]\n",
    "        qtl_file_2 = qtl_files[j]\n",
    "        qtl1 = qtl_names[i]\n",
    "        qtl2 = qtl_names[j]\n",
    "        c = f'python /mnt/hpc/home/xuxinran/code/methy/nanopore_RNA/downsteam/leadSNP_dis_LD.py -c {chrom_size_file} -d {work_dir} --geno {bfile_path} --gff {gene_bed_file} -b bim_dict.pkl --qf1 {qtl_file_1} --qf2 {qtl_file_2} --q1 {qtl1} --q2 {qtl2}'\n",
    "        res.append(c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存res为txt文件\n",
    "with open('/mnt/hpc/home/xuxinran/DirectSeq/8_downsteam/leadSNP_dis_LD/run.sh', 'w') as f:\n",
    "    for item in res:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_all_qtl_pairs(qtl_files, qtl_names, gene_bed_file, chrom_size_file, work_dir, bim_dict, bfile_path):\n",
    "    \"\"\"\n",
    "    对所有QTL进行两两分析。\n",
    "\n",
    "    Args:\n",
    "        qtl_files: 包含所有QTL文件路径的列表。\n",
    "        qtl_names: 包含所有QTL名称的列表。\n",
    "        gene_bed_file: 基因bed文件路径。\n",
    "        chrom_size_file: 染色体大小文件路径。\n",
    "        work_dir: 工作目录。\n",
    "        bim_dict: bim文件字典。\n",
    "        bfile_path: bfile文件路径。\n",
    "    \"\"\"\n",
    "\n",
    "    # 使用itertools.combinations生成所有可能的QTL对\n",
    "    for i, j in itertools.combinations(range(len(qtl_names)), 2):\n",
    "        qtl_file_1 = qtl_files[i]\n",
    "        qtl_file_2 = qtl_files[j]\n",
    "        qtl1 = qtl_names[i]\n",
    "        qtl2 = qtl_names[j]\n",
    "        print(f'Analyzing {qtl1} and {qtl2}...')\n",
    "        analyze_leadQTL_distance(qtl_file_1, qtl_file_2, qtl1, qtl2, gene_bed_file, chrom_size_file, work_dir, bim_dict, bfile_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze_all_qtl_pairs(qtl_files, qtl_names, gene_bed_file, chrom_size_file, work_dir, bim_dict, bfile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 矫正rsID为bim中的SNP name的文件\n",
    "\n",
    "bim_file = \"/mnt/hpc/home/xuxinran/huvec_genotype/huvec_imputed.bim\"\n",
    "bim_df = pd.read_csv(bim_file, sep='\\t', header=None, usecols=[1])\n",
    "bim_df.columns = ['SNP_name']\n",
    "# 使用str.contains()方法判断是否包含\":\"或\"_\"\n",
    "mask_colon = bim_df['SNP_name'].str.contains(\":\")\n",
    "mask_underscore = bim_df['SNP_name'].str.contains(\"_\")\n",
    "# 使用str.split()方法分割字符串\n",
    "bim_df.loc[mask_colon, 'rsID'] = bim_df.loc[mask_colon, 'SNP_name'].str.split(\":\", n=1, expand=True)[0]\n",
    "bim_df.loc[mask_underscore, 'rsID'] = bim_df.loc[mask_underscore, 'SNP_name'].str.split(\"_\", n=2, expand=True).apply(lambda x: \"_\".join(x[:2]), axis=1)\n",
    "# 处理不包含\":\"或\"_\"的SNP_name\n",
    "bim_df.loc[~(mask_colon | mask_underscore), 'rsID'] = bim_df.loc[~(mask_colon | mask_underscore), 'SNP_name']\n",
    "\n",
    "bim_dict = pd.Series(bim_df['SNP_name'].values, index=bim_df['rsID']).to_dict()\n",
    "\n",
    "# 将字典保存到指定路径\n",
    "with open('bim_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(bim_dict, f)\n",
    "\n",
    "# 读取\n",
    "with open('bim_dict.pkl', 'rb') as f:\n",
    "    bim_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_plink_ld(bfile_path, rs1, rs2):\n",
    "    if rs1 == rs2:\n",
    "        return 1, 1\n",
    "    else:\n",
    "        command = [\n",
    "                'plink',\n",
    "                '--bfile', bfile_path,\n",
    "                '--ld', rs1, rs2\n",
    "            ]\n",
    "        res = run(command, capture_output=True, text=True, check=True)\n",
    "        filtered_ld_list = extract_r_sq_d(res.stdout)\n",
    "        r2 ,d = filtered_ld_list[0][0], filtered_ld_list[0][1]\n",
    "        return r2 ,d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_r_sq_d(plink_output):\n",
    "    r_sq_d_list = []\n",
    "    pattern = r\"R-sq = (.*)\\s+D' = (.*)\\n\\n\"  # 匹配R-sq和D'的正则表达式\n",
    "    matches = re.findall(pattern, plink_output)\n",
    "    for match in matches:\n",
    "        r_sq = float(match[0])\n",
    "        d = float(match[1])\n",
    "        r_sq_d_list.append((r_sq, d))\n",
    "    filtered_ld_list = filter_ld_results(r_sq_d_list)\n",
    "    return filtered_ld_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ld_results(r_sq_d_list, low_threshold=0.2, high_threshold=0.8):\n",
    "\n",
    "    if len(r_sq_d_list) == 1:  # 如果只有一个结果，则直接保留\n",
    "        return r_sq_d_list\n",
    "\n",
    "    filtered_ld_list = []\n",
    "    for r_sq, d in r_sq_d_list:\n",
    "        if r_sq < low_threshold and d < low_threshold:\n",
    "            continue  # 剔除R-sq和D'都很小的结果\n",
    "        elif r_sq > high_threshold and d > high_threshold:\n",
    "            filtered_ld_list.append((r_sq, d))  # 保留R-sq和D'都很大的结果\n",
    "        elif (r_sq > low_threshold and d < low_threshold) or (r_sq < low_threshold and d > low_threshold):\n",
    "            continue  # 剔除R-sq和D'一个大一个小的结果\n",
    "        else:\n",
    "            filtered_ld_list.append((r_sq, d)) # 保留其他结果\n",
    "\n",
    "    if len(filtered_ld_list) > 0:\n",
    "        return filtered_ld_list\n",
    "    else:\n",
    "        # 如果所有结果都被剔除，则选择一个相对合理的结果\n",
    "        reasonable_result = None\n",
    "        for r_sq, d in r_sq_d_list:\n",
    "            if reasonable_result is None:\n",
    "                reasonable_result = (r_sq, d)\n",
    "            elif abs(r_sq - d) < abs(reasonable_result[0] - reasonable_result[1]):  # 选择R-sq和D'差异最小的结果\n",
    "                reasonable_result = (r_sq, d)\n",
    "        return [reasonable_result]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_ld_to_dataframe(res1_df, bfile_path):\n",
    "\n",
    "    r2_list = []\n",
    "    d_list = []\n",
    "\n",
    "    for index, row in res1_df.iterrows():\n",
    "        rs1 = row['rsID_1']\n",
    "        rs2 = row['rsID_2']\n",
    "        r2, d = run_plink_ld(bfile_path, rs1, rs2)\n",
    "        r2_list.append(r2)\n",
    "        d_list.append(d)\n",
    "\n",
    "    res1_df['R_sq'] = r2_list\n",
    "    res1_df['D'] = d_list\n",
    "\n",
    "    return res1_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "methy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
